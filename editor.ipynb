{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/__init__.py\n",
    "# To reference the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/svm.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/svm.py\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "class HuberSVM:\n",
    "    def __init__(self, h=None):\n",
    "        self.beta_vals = None\n",
    "        self.cache = {}\n",
    "        self.h = h\n",
    "        \n",
    "    def fit(self, X_train, y_train, lam=1, **config):\n",
    "        \"\"\"Fit the model\n",
    "        \"\"\"\n",
    "        # Initialize\n",
    "        n, d = X_train.shape\n",
    "        beta_init = np.zeros(n)\n",
    "        theta_init = np.zeros(n)\n",
    "        K = None\n",
    "        sigma = None\n",
    "        order = None\n",
    "        eta_init = None\n",
    "        max_iter = None\n",
    "        eps = None\n",
    "        kernel_choice = None\n",
    "        plot = False\n",
    "        \n",
    "        if 'kernel_choice' in config:\n",
    "            kernel_choice = config['kernel_choice']\n",
    "        else:\n",
    "            kernel_choice = 'linear'\n",
    "        \n",
    "        if 'sigma' in config:\n",
    "            sigma = config['sigma']\n",
    "        elif sigma is None and kernel_choice=='rbf':\n",
    "            # Set sigma based on pairwise distances.\n",
    "            dists = sklearn.metrics.pairwise.pairwise_distances(X_train).reshape(-1)\n",
    "            sigma = np.median(dists)\n",
    "            \n",
    "        if 'order' in config:\n",
    "            order = config['order']\n",
    "        elif order is None and kernel_choice=='poly':\n",
    "            order = 2\n",
    "        \n",
    "        if 'max_iter' in config:\n",
    "            max_iter = config['max_iter']\n",
    "        else:\n",
    "            max_iter = 50\n",
    "            \n",
    "        if 'eps' in config:\n",
    "            eps = config['eps']\n",
    "        else:\n",
    "            eps = 1e-5\n",
    "            \n",
    "        if 'eta_init' in config:\n",
    "            eta_init = config['eta_init']\n",
    "            \n",
    "        if 'plot' in config:\n",
    "            plot = config['plot']\n",
    "        \n",
    "        # Main Loop\n",
    "        K = self.gram(X_train, X_train, **{'kernel_choice': kernel_choice, 'sigma': sigma, \n",
    "                                           'order': order})\n",
    "        if 'eta_init' not in config:\n",
    "            # Set eta_init based on an upper bound on the Lipschitz constant.\n",
    "            eta_init = 1 / scipy.linalg.eigh(2 / n * np.dot(K, K) + 2 * lam * K, eigvals=(n - 1, n - 1),\n",
    "                                             eigvals_only=True)[0]\n",
    "        self.beta_vals = self.fastgradalgo(beta_init, theta_init, K, y_train, lam, eta_init, max_iter, eps)\n",
    "        if plot:\n",
    "            ax = self.objective_plot(self.beta_vals, K, y_train, lam)\n",
    "            self.cache['plot'] = ax\n",
    "        self.cache['kernel_choice'] = kernel_choice\n",
    "        self.cache['sigma'] = sigma\n",
    "        self.cache['order'] = order\n",
    "        self.cache['eta_init'] = eta_init\n",
    "        self.cache['lambda'] = lam\n",
    "    \n",
    "    def gram(self, X, Z, **config):\n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "        - X: matrix with observations as rows\n",
    "        - Z: Another matrix with observations as rows\n",
    "        - Sigma: kernel bandwidth\n",
    "        Output: Gram matrix\n",
    "        \"\"\"  \n",
    "        if Z is None:\n",
    "            Z = X\n",
    "        if config['kernel_choice'] == 'rbf':\n",
    "            return np.exp(-1/(2*config['sigma']**2)*((np.linalg.norm(X, axis=1)**2)[:, np.newaxis] + (np.linalg.norm(Z, axis=1)**2)[np.newaxis, :] - 2*np.dot(X, Z.T)))\n",
    "        elif config['kernel_choice'] == 'linear':\n",
    "            return X.dot(Z.T)\n",
    "        elif config['kernel_choice'] == 'poly':\n",
    "            return (X.dot(Z.T) + 1)**config['order']\n",
    "        else:\n",
    "            print(\"Kernel Not Implemented\")\n",
    "            return X\n",
    "        \n",
    "    def fastgradalgo(self, beta_init, theta_init, K, y, lam, eta_init, max_iter, eps):\n",
    "        beta = beta_init\n",
    "        theta = theta_init\n",
    "        eta = eta_init\n",
    "        grad_theta = self.grad(theta, K, y, lam)\n",
    "        grad_beta = self.grad(beta, K, y, lam)\n",
    "        beta_vals = beta\n",
    "        iter = 0\n",
    "        while iter < max_iter and np.linalg.norm(grad_beta) > eps:\n",
    "            eta = self.bt_line_search(theta, K, y, lam, eta=eta)\n",
    "            beta_new = theta - eta*grad_theta\n",
    "            theta = beta_new + iter/(iter+3)*(beta_new-beta)\n",
    "            grad_theta = self.grad(theta, K, y, lam)\n",
    "            grad_beta = self.grad(beta, K, y, lam)\n",
    "            beta = beta_new\n",
    "            iter += 1\n",
    "            if iter % 1 == 0:\n",
    "                beta_vals = np.vstack((beta_vals, beta_new))\n",
    "        return beta_vals\n",
    "    \n",
    "    def obj(self, beta, K, y, lam, h=0.5):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - beta: Vector to be optimized\n",
    "        - K: Gram matrix consisting of evaluations of the kernel k(x_i, x_j) for i,j=1,...,n\n",
    "        - y: Labels y_1,...,y_n corresponding to x_1,...,x_n\n",
    "        - lam: Penalty parameter lambda\n",
    "        Output:\n",
    "        - Value of the objective function at beta\n",
    "        \"\"\"\n",
    "        if self.h is not None:\n",
    "            h = self.h\n",
    "        cost_vector = np.zeros(y.shape[0])\n",
    "        t = K.dot(beta)\n",
    "        yt = y * t\n",
    "        cost_vector[yt < 1-h] = (1 - yt[yt < 1 - h])\n",
    "        cost_vector[np.absolute(1 - yt) <= h] = (((1 + h - yt[np.absolute(1 - yt) <= h])**2) / (4 * h))\n",
    "        return (1/y.shape[0]) * np.sum(cost_vector) + lam * beta.dot(K).dot(beta)\n",
    "\n",
    "    def grad(self, beta, K, y, lam, h=0.5):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - beta: Vector to be optimized\n",
    "        - K: Gram matrix consisting of evaluations of the kernel k(x_i, x_j) for i,j=1,...,n\n",
    "        - y: Labels y_1,...,y_n corresponding to x_1,...,x_n\n",
    "        - lam: Penalty parameter lambda\n",
    "        Output:\n",
    "        - Value of the gradient at beta\n",
    "        \"\"\"\n",
    "        if self.h is not None:\n",
    "            h = self.h\n",
    "        grad_matrix = np.zeros((y.shape[0], y.shape[0]))\n",
    "        t = K.dot(beta)\n",
    "        yt = y * t\n",
    "        grad_matrix[yt < 1 - h] = (-y[:, np.newaxis] * K)[yt < 1 - h]\n",
    "        grad_matrix[np.absolute(1 - yt) <= h] = (((-2 / (4 * h)) * (1 + h - yt))[:, np.newaxis] * (y[:, np.newaxis] * K))[np.absolute(1 - yt) <= h]\n",
    "        return 1/y.shape[0] * np.sum(grad_matrix, axis=0) + 2 * lam * K.dot(beta)\n",
    "\n",
    "    def bt_line_search(self, beta, K, y, lam, eta=1, alpha=0.5, betaparam=0.8, max_iter=100):\n",
    "        grad_beta = self.grad(beta, K, y, lam)\n",
    "        norm_grad_beta = np.linalg.norm(grad_beta)\n",
    "        found_eta = 0\n",
    "        iter = 0\n",
    "        while found_eta == 0 and iter < max_iter:\n",
    "            if self.obj(beta - eta * grad_beta, K, y, lam) < \\\n",
    "                            self.obj(beta, K, y, lam) - alpha * eta * norm_grad_beta ** 2:\n",
    "                found_eta = 1\n",
    "            elif iter == max_iter-1:\n",
    "                raise ('Max number of iterations of backtracking line search reached')\n",
    "            else:\n",
    "                eta *= betaparam\n",
    "                iter += 1\n",
    "        return eta\n",
    "    \n",
    "    def objective_plot(self, betas, K, y, lam):\n",
    "        num_points = np.size(betas, 0)\n",
    "        objs = np.zeros(num_points)\n",
    "        for i in range(0, num_points):\n",
    "            objs[i] = self.obj(betas[i, :], K, y, lam)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.array(range(num_points)), objs, c='red')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Objective value')\n",
    "        ax.set_title('Objective value vs. iteration when lambda=' + str(lam))\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting examples/sim_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%file examples/sim_demo.py\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from src.svm import HuberSVM\n",
    "\n",
    "def sim_demo(verbose=True, plot_contour=False):\n",
    "    def evaluate(beta, X_train, X_test, y_test, kernel, **kwargs):\n",
    "        n_test = len(y_test)\n",
    "        y_pred = np.zeros(n_test)\n",
    "        y_vals = np.zeros(n_test)\n",
    "        for i in range(n_test):\n",
    "            y_vals[i] = np.dot(kernel(X_train, X_test[i, :].reshape(1, -1), **kwargs).reshape(-1), beta)\n",
    "        y_pred = np.sign(y_vals)\n",
    "        return np.mean(y_pred != y_test), y_vals  # return error and values from before applying cutoff\n",
    "    \n",
    "    def evaluate_plot(betas, kernel, plot_contour, **kwargs):\n",
    "        error, test_values = evaluate(betas[-1, :], X_train, X_test, y_test, kernel, **kwargs)\n",
    "        print('Misclassification error when lambda =', kwargs['lambda'], ':', error)\n",
    "        ax = None\n",
    "        if plot_contour:\n",
    "            Zs = np.c_[xx.ravel(), yy.ravel()]\n",
    "            Z = evaluate(betas[-1, :], X_train, Zs, [0]*len(Zs), kernel, **kwargs)[1]\n",
    "            # Put the result into a color plot\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax = plt.subplot()\n",
    "            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "            # Plot also the training points\n",
    "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                       edgecolors='k')\n",
    "            # and testing points\n",
    "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                       edgecolors='k', alpha=0.2)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "        return error, test_values, ax\n",
    "        \n",
    "    X, y = sklearn.datasets.make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "    y_train = 2*y_train - 1\n",
    "    y_test = 2*y_test - 1\n",
    "    if verbose:\n",
    "        print('Number of training examples:', X_train.shape[0])\n",
    "        print('Number of test examples:', X_test.shape[0])\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    if plot_contour:\n",
    "        # Plot the training points\n",
    "        ax = plt.subplot()\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.1,\n",
    "                   edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "    mysvm = HuberSVM()\n",
    "    mysvm.fit(X_train, y_train, lam=0.8, **{'plot': True, 'kernel_choice': 'rbf', 'sigma': 1, 'max_iter': 10})\n",
    "    beta_vals = mysvm.beta_vals\n",
    "    train_cache = mysvm.cache\n",
    "    errors, test_vals, contour_ax = evaluate_plot(beta_vals, mysvm.gram, plot_contour, **train_cache)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    sim_demo(verbose=True, plot_contour=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting examples/real_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%file examples/real_demo.py\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from src.svm import HuberSVM\n",
    "    \n",
    "def real_demo(verbose=True, plot_progress=True):\n",
    "    def filter_pair(label_pair, x, y):\n",
    "        \"\"\" Filter a multi-class dataset into binary dataset given a label pair.\n",
    "        \"\"\"\n",
    "        mask = np.isin(y, label_pair)\n",
    "        x_bin, y_bin = x[mask].copy(), y[mask].copy()\n",
    "        y_bin[y_bin==label_pair[0]] = 1.0\n",
    "        y_bin[y_bin==label_pair[1]] = -1.0\n",
    "        return x_bin, y_bin\n",
    "\n",
    "    def evaluate(beta, X_train, X_test, kernel, **kwargs):\n",
    "        n_test = X_test.shape[0]\n",
    "        y_pred = np.zeros(n_test)\n",
    "        y_vals = np.zeros(n_test)\n",
    "        for i in range(n_test):\n",
    "            y_vals[i] = np.dot(kernel(X_train, X_test[i, :].reshape(1, -1), **kwargs).reshape(-1), beta)\n",
    "        return y_vals\n",
    "\n",
    "    def train_predict(X_train, y_train, X_test, y_test, lam, method='ovo', **config):\n",
    "        error = None\n",
    "        label_list = np.unique(y_train)\n",
    "        if method == 'ovo':\n",
    "            pred_list = []\n",
    "            label_pair_list = list(itertools.combinations(label_list, 2))\n",
    "            for label_pair in tqdm(label_pair_list):\n",
    "                X_train_bin, y_train_bin = filter_pair(label_pair, X_train, y_train)\n",
    "                mylinearsvm = HuberSVM()\n",
    "                mylinearsvm.fit(X_train_bin, y_train_bin, lam, **config)\n",
    "                beta_vals, train_cache = mylinearsvm.beta_vals, mylinearsvm.cache\n",
    "                if config['plot']:\n",
    "                    plt.show(train_cache['plot'])\n",
    "                scores = evaluate(beta_vals[-1, :], X_train_bin, X_test, mylinearsvm.gram, **train_cache)\n",
    "                y_pred_bin = np.zeros_like(y_test) + label_pair[-1]\n",
    "                y_pred_bin[scores >= 0] = label_pair[0]\n",
    "                pred_list.append(y_pred_bin)\n",
    "            test_preds = np.array([mode(pi).mode[0] for pi in np.array(pred_list, dtype=np.int64).T])\n",
    "            error = np.mean(test_preds != y_test)\n",
    "        elif method == 'ovr':\n",
    "            score_list = []\n",
    "            for label in tqdm(label_list):\n",
    "                y_train_bin = np.zeros_like(y_train) - 1\n",
    "                y_train_bin[y_train == label] = 1\n",
    "                mylinearsvm = HuberSVM()\n",
    "                mylinearsvm.fit(X_train, y_train_bin, lam, **config)\n",
    "                beta_vals, train_cache = mylinearsvm.beta_vals, mylinearsvm.cache\n",
    "                if config['plot']:\n",
    "                    plt.show(train_cache['plot'])\n",
    "                scores = evaluate(beta_vals[-1, :], X_train, X_test, mylinearsvm.gram, **train_cache)\n",
    "                score_list.append(scores)\n",
    "            test_preds = np.argmax(np.stack(score_list, axis=1), axis=1)\n",
    "            error = np.mean(test_preds != y_test)\n",
    "        else:\n",
    "            print(\"Method Not Implemented\")\n",
    "        return error, beta_vals\n",
    "    \n",
    "    print(\"Download the dataset...\")\n",
    "    digits = load_digits() \n",
    "    num_images = digits.images.shape[0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(digits.images.reshape(num_images, -1), digits.target, test_size=.4, random_state=42)\n",
    "    #Standardizing the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    if verbose:\n",
    "        print('Number of training examples:', X_train.shape)\n",
    "        print('Number of test examples:', X_test.shape)\n",
    "    lambda_list = [1]\n",
    "    for l in lambda_list:\n",
    "        error, beta_vals = train_predict(X_train, y_train, X_test, y_test, lam=l, method='ovr', **{'kernel_choice': 'linear', 'sigma': 1, 'plot': plot_progress, 'max_iter': 50})\n",
    "        print(\"Lambda = %0.4f. Misclassification Error = %0.4f\" %(l, error))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    real_demo(verbose=True, plot_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
